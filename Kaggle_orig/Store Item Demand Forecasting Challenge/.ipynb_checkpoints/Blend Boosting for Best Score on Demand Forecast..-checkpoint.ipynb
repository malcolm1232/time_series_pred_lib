{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://www.kaggle.com/code/hikmetsezen/base-model-with-lightgbm-on-demand-forecasting/notebook\n",
    "#### Overview Notes:\n",
    "1. Model: LightGBM\n",
    "2. Why it got top 1: \n",
    "  - Mainly: Good, logical Feature Engineering\n",
    "  \n",
    "#### Logic Notes\n",
    "1. Expand Year Index to ['day'], ['month'], ['Year'], ['dayofweek'], Engineering Feature includes:\n",
    "  - dataframe['month'] = dataframe.date.dt.month\n",
    "  - dataframe['day_of_month'] = dataframe.date.dt.day\n",
    "  - dataframe['day_of_year'] = dataframe.date.dt.dayofyear\n",
    "  - dataframe['week_of_year'] = dataframe.date.dt.weekofyear\n",
    "  - dataframe['day_of_week'] = dataframe.date.dt.dayofweek + 1\n",
    "  - dataframe['year'] = dataframe.date.dt.year\n",
    "  - dataframe['is_wknd'] = dataframe.date.dt.weekday // 4\n",
    "  - dataframe['is_month_start'] = dataframe.date.dt.is_month_start.astype(int)\n",
    "  - dataframe['is_month_end'] = dataframe.date.dt.is_month_end.astype(int)\n",
    "  - dataframe['quarter'] = dataframe.date.dt.quarter\n",
    "  - dataframe['week_block_num'] = [int(x) for x in np.floor((dataframe.date - pd.to_datetime('2012-12-31')).dt.days / 7) + 1]\n",
    "  - dataframe['quarter_block_num'] = (dataframe['year'] - 2013) * 4 + dataframe['quarter']\n",
    "  - dataframe['week_of_month'] = dataframe['week_of_year'].values // 4.35\n",
    "2. Use of nonlinear growth rate projection\n",
    "#### Technical Notes:\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-finding",
   "metadata": {
    "papermill": {
     "duration": 0.012982,
     "end_time": "2021-05-03T21:04:18.408266",
     "exception": false,
     "start_time": "2021-05-03T21:04:18.395284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Store Item Demand Forecasting Challenge :\n",
    "\n",
    "Dataset of the \"Store Item Demand Forecasting Challenge\" (https://www.kaggle.com/c/demand-forecasting-kernels-only/) is a time series related case study for me during my \"Data Science and Machine Learning\" bootcamp journey.\n",
    "\n",
    "I develop a LigthGBM model including advanced feature engineering about different type approaches ie. smoothings, lag/shift injections on series, linear/nonlinear forecasting projections, encodings, model tuning etc. My notebook is able to reach public scores between 13.84000 - 13.87000. I would like to share it and any discussion/comment is welcome.\n",
    "\n",
    "I use couple of shared notebooks to improve my notebook, and a list of them here:\n",
    "* https://www.kaggle.com/ashishpatel26/keeping-it-simple-by-xyzt\n",
    "* https://www.kaggle.com/miladdoostan/handling-outliers-feature-engineering-lgbm\n",
    "* https://www.kaggle.com/ymatioun/simple-lightgbm\n",
    "* https://www.kaggle.com/elitcohen/store-sales-eda-and-linear-drift-prediction\n",
    "* https://www.kaggle.com/abhilashawasthi/feature-engineering-lgb-model\n",
    "* https://www.kaggle.com/ekrembayar/store-item-demand-forecasting-with-lgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numerical-cookie",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:18.441436Z",
     "iopub.status.busy": "2021-05-03T21:04:18.439229Z",
     "iopub.status.idle": "2021-05-03T21:04:18.442887Z",
     "shell.execute_reply": "2021-05-03T21:04:18.444604Z"
    },
    "papermill": {
     "duration": 0.023835,
     "end_time": "2021-05-03T21:04:18.444933",
     "exception": false,
     "start_time": "2021-05-03T21:04:18.421098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install lightgbm==2.3.1\n",
    "# import lightgbm\n",
    "# lightgbm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-program",
   "metadata": {
    "papermill": {
     "duration": 0.012241,
     "end_time": "2021-05-03T21:04:18.470406",
     "exception": false,
     "start_time": "2021-05-03T21:04:18.458165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Importing Libraries and Loading Datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "czech-composition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:18.509530Z",
     "iopub.status.busy": "2021-05-03T21:04:18.506796Z",
     "iopub.status.idle": "2021-05-03T21:04:22.104187Z",
     "shell.execute_reply": "2021-05-03T21:04:22.103498Z"
    },
    "papermill": {
     "duration": 3.61855,
     "end_time": "2021-05-03T21:04:22.104337",
     "exception": false,
     "start_time": "2021-05-03T21:04:18.485787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import time\n",
    "start_time = time.time()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load datasets\n",
    "train = pd.read_csv(r'../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'], index_col=['date'])\n",
    "test = pd.read_csv(r'../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'], index_col=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-paris",
   "metadata": {
    "papermill": {
     "duration": 0.012532,
     "end_time": "2021-05-03T21:04:22.130594",
     "exception": false,
     "start_time": "2021-05-03T21:04:22.118062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Non-Linear Growth Rate Projection for 2018 :\n",
    "#### *(data pre-processing)*\n",
    "\n",
    "The next cell is adapted from a notebook of https://www.kaggle.com/ashishpatel26/keeping-it-simple-by-xyzt. Basically, with a nonlinear growth rate projection the sales prediction for 2018 is calculated with a high precision on reference tables of monthly, store and day of week sales by considering a full year. Quite smart solution! I attempt similar solutions mostly focus on first three months of year, but I could not achieve better than ~14.1 even though I fully calibrate day of week between each year. I use this sale prediction to expand volume of train dataset with also test dataset in the LightGBM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regulated-leisure",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:22.171800Z",
     "iopub.status.busy": "2021-05-03T21:04:22.166117Z",
     "iopub.status.idle": "2021-05-03T21:04:30.736815Z",
     "shell.execute_reply": "2021-05-03T21:04:30.736206Z"
    },
    "papermill": {
     "duration": 8.593361,
     "end_time": "2021-05-03T21:04:30.736974",
     "exception": false,
     "start_time": "2021-05-03T21:04:22.143613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sales_prediction():\n",
    "\n",
    "    # Expand dataframe with more useful columns\n",
    "    def expand_df(dataframe):\n",
    "        dataframe['day'] = dataframe.index.day\n",
    "        dataframe['month'] = dataframe.index.month\n",
    "        dataframe['year'] = dataframe.index.year\n",
    "        dataframe['dayofweek'] = dataframe.index.dayofweek\n",
    "        return dataframe\n",
    "\n",
    "    data = expand_df(train)\n",
    "\n",
    "    # Only data 2015 and after is used\n",
    "    new_data = data.loc[data.year >= 2015]\n",
    "    grand_avg = new_data.sales.mean()\n",
    "\n",
    "    # Day of week - Item Look up table\n",
    "    dow_item_table = pd.pivot_table(new_data, index='dayofweek', columns='item', values='sales', aggfunc=np.mean)\n",
    "\n",
    "    # Month pattern\n",
    "    month_table = pd.pivot_table(new_data, index='month', values='sales', aggfunc=np.mean) / grand_avg\n",
    "\n",
    "    # Store pattern\n",
    "    store_table = pd.pivot_table(new_data, index='store', values='sales', aggfunc=np.mean) / grand_avg\n",
    "\n",
    "    # weighted growth rate\n",
    "    year_table = pd.pivot_table(data, index='year', values='sales', aggfunc=np.mean) / grand_avg\n",
    "    years = np.arange(2013, 2019)\n",
    "    annual_growth = np.poly1d(np.polyfit(years[:-1], year_table.values.squeeze(), 2, w=np.exp((years - 2018) / 10)[:-1]))\n",
    "\n",
    "    pred_sales = []\n",
    "    for _, row in test.iterrows():\n",
    "        dow, month, year = row.name.dayofweek, row.name.month, row.name.year\n",
    "        item, store = row['item'], row['store']\n",
    "        base_sales = dow_item_table.at[dow, item]\n",
    "        mul = month_table.at[month, 'sales'] * store_table.at[store, 'sales']\n",
    "        pred_sales.append(int(np.round(base_sales * mul * annual_growth(year), 0)))\n",
    "\n",
    "    return pred_sales\n",
    "\n",
    "\n",
    "# extending train dataset with test dataset by sale prediction for 2018\n",
    "test['sales'] = sales_prediction()\n",
    "train = train.loc[train.index.year >= 2015, :] # use only data after 2015\n",
    "df = pd.concat([train, test], sort=False)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-bones",
   "metadata": {
    "papermill": {
     "duration": 0.014142,
     "end_time": "2021-05-03T21:04:30.764406",
     "exception": false,
     "start_time": "2021-05-03T21:04:30.750264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating Datetime Related Features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welsh-permission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:30.809426Z",
     "iopub.status.busy": "2021-05-03T21:04:30.808062Z",
     "iopub.status.idle": "2021-05-03T21:04:31.981562Z",
     "shell.execute_reply": "2021-05-03T21:04:31.982247Z"
    },
    "papermill": {
     "duration": 1.204741,
     "end_time": "2021-05-03T21:04:31.982486",
     "exception": false,
     "start_time": "2021-05-03T21:04:30.777745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create feature from datetime columns\n",
    "def create_date_features(dataframe):\n",
    "    dataframe['month'] = dataframe.date.dt.month\n",
    "    dataframe['day_of_month'] = dataframe.date.dt.day\n",
    "    dataframe['day_of_year'] = dataframe.date.dt.dayofyear\n",
    "    dataframe['week_of_year'] = dataframe.date.dt.weekofyear\n",
    "    dataframe['day_of_week'] = dataframe.date.dt.dayofweek + 1\n",
    "    dataframe['year'] = dataframe.date.dt.year\n",
    "    dataframe['is_wknd'] = dataframe.date.dt.weekday // 4\n",
    "    dataframe['is_month_start'] = dataframe.date.dt.is_month_start.astype(int)\n",
    "    dataframe['is_month_end'] = dataframe.date.dt.is_month_end.astype(int)\n",
    "    dataframe['quarter'] = dataframe.date.dt.quarter\n",
    "    dataframe['week_block_num'] = [int(x) for x in np.floor((dataframe.date - pd.to_datetime('2012-12-31')).dt.days / 7) + 1]\n",
    "    dataframe['quarter_block_num'] = (dataframe['year'] - 2013) * 4 + dataframe['quarter']\n",
    "    dataframe['week_of_month'] = dataframe['week_of_year'].values // 4.35\n",
    "    return dataframe\n",
    "                                                                                                                             \n",
    "                                                                                                                                              \n",
    "df = create_date_features(df)                                                                                                                 \n",
    "                                                                                                                                              \n",
    "# day labeling features                                                                       \n",
    "df['is_Mon'] = np.where(df['day_of_week'] == 1, 1, 0)                                                                                            \n",
    "df['is_Tue'] = np.where(df['day_of_week'] == 2, 1, 0)                                                                                         \n",
    "df['is_Wed'] = np.where(df['day_of_week'] == 3, 1, 0)                                                                                         \n",
    "df['is_Thu'] = np.where(df['day_of_week'] == 4, 1, 0)                                                                                         \n",
    "df['is_Fri'] = np.where(df['day_of_week'] == 5, 1, 0)                                                                                         \n",
    "df['is_Sat'] = np.where(df['day_of_week'] == 6, 1, 0)                                                                                         \n",
    "df['is_Sun'] = np.where(df['day_of_week'] == 7, 1, 0)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-evolution",
   "metadata": {
    "papermill": {
     "duration": 0.014981,
     "end_time": "2021-05-03T21:04:32.012427",
     "exception": false,
     "start_time": "2021-05-03T21:04:31.997446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating Sale Aggregation Based Feature :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "public-track",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:32.048585Z",
     "iopub.status.busy": "2021-05-03T21:04:32.047634Z",
     "iopub.status.idle": "2021-05-03T21:04:38.784211Z",
     "shell.execute_reply": "2021-05-03T21:04:38.783364Z"
    },
    "papermill": {
     "duration": 6.758898,
     "end_time": "2021-05-03T21:04:38.784451",
     "exception": false,
     "start_time": "2021-05-03T21:04:32.025553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generating some new features from aggregation of sales within different time frames\n",
    "feat_list = ['day_of_week', 'week_of_month', 'week_of_year', 'month', 'quarter', 'is_wknd'] + ['day_of_week', 'week_of_month']\n",
    "shift_values = [0, 0, 0, 0, 0, 0, 12, 12]\n",
    "for time_item, shift_val in zip(feat_list, shift_values):\n",
    "    grouped_df = df.groupby(['store', 'item', time_item])['sales'].expanding().mean().shift(shift_val).bfill().reset_index()\n",
    "    grouped_df.columns = ['store', 'item', time_item, 'date', time_item + f'_ex_avg_sale{str(shift_val)}']\n",
    "    grouped_df = grouped_df.sort_values(by=['item', 'store', 'date'])\n",
    "    df[time_item + f'_ex_avg_sale{str(shift_val)}'] = grouped_df[time_item + f'_ex_avg_sale{str(shift_val)}'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-ground",
   "metadata": {
    "papermill": {
     "duration": 0.013706,
     "end_time": "2021-05-03T21:04:38.812026",
     "exception": false,
     "start_time": "2021-05-03T21:04:38.798320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating Smoothing based Features with Lag/Shift, Rolling Mean and Exponentially Weighted Techniques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "federal-television",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:38.848174Z",
     "iopub.status.busy": "2021-05-03T21:04:38.847324Z",
     "iopub.status.idle": "2021-05-03T21:04:59.922069Z",
     "shell.execute_reply": "2021-05-03T21:04:59.921352Z"
    },
    "papermill": {
     "duration": 21.094044,
     "end_time": "2021-05-03T21:04:59.922235",
     "exception": false,
     "start_time": "2021-05-03T21:04:38.828191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure dataset sorted with original order                                                  \n",
    "df.sort_values(by=['item', 'store', 'date'], axis=0, inplace=True) \n",
    "\n",
    "\n",
    "#generating some noise                                                                   \n",
    "def random_noise(dataframe):                                                                                                                  \n",
    "    return np.random.normal(scale=0.01, size=(len(dataframe),))    \n",
    "\n",
    "\n",
    "# Lag/Shifted Features                                                                                                                                                      \n",
    "# generating laggy features with different time windows                                                                                                                                 \n",
    "def lag_features(dataframe, lags):                                                                                                            \n",
    "    dataframe = dataframe.copy()                                                                                                              \n",
    "    for lag in lags:                                                                                                                          \n",
    "        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"item\", \"store\"])['sales'].transform(lambda x: x.shift(lag)) + random_noise(dataframe)                                                                                 \n",
    "    return dataframe                                                                                                                          \n",
    "                                                                                                                                              \n",
    "                                                                                                                                              \n",
    "df = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])                                                                       \n",
    "                                                                                                                                 \n",
    "\n",
    "    \n",
    "# Rolling Mean Features                                                                                                                       \n",
    "def roll_mean_features(dataframe, windows):                                                                                                   \n",
    "    dataframe = dataframe.copy()                                                                                                              \n",
    "    for window in windows:                                                                                                                    \n",
    "        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"item\", \"store\"])['sales'].\\\n",
    "        transform(lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(dataframe)            \n",
    "    return dataframe                                                                                                                          \n",
    "                                                                                                                                              \n",
    "                                                          \n",
    "df = roll_mean_features(df, [91, 182, 365, 546, 730])                                                                                         \n",
    "                                                                                                                                              \n",
    "\n",
    "    \n",
    "# Exponentially Weighted Mean Features                                                                                                        \n",
    "def ewm_features(dataframe, alphas, lags):                                                                                                    \n",
    "    dataframe = dataframe.copy()                                                                                                              \n",
    "    for alpha in alphas:                                                                                                                      \n",
    "        for lag in lags:                                                                                                                      \n",
    "            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n",
    "            dataframe.groupby([\"item\", \"store\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())                       \n",
    "    return dataframe                                                                                                                          \n",
    "                                                                                                                                              \n",
    "                                                                                                                                              \n",
    "alphas = [0.95, 0.9, 0.8, 0.7, 0.5]                                             \n",
    "lags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n",
    "df = ewm_features(df, alphas, lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-treasury",
   "metadata": {
    "papermill": {
     "duration": 0.016637,
     "end_time": "2021-05-03T21:04:59.954441",
     "exception": false,
     "start_time": "2021-05-03T21:04:59.937804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Final step for Data preparation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confirmed-maker",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:04:59.994698Z",
     "iopub.status.busy": "2021-05-03T21:04:59.993822Z",
     "iopub.status.idle": "2021-05-03T21:05:00.753448Z",
     "shell.execute_reply": "2021-05-03T21:05:00.752121Z"
    },
    "papermill": {
     "duration": 0.783935,
     "end_time": "2021-05-03T21:05:00.753688",
     "exception": false,
     "start_time": "2021-05-03T21:04:59.969753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of feature engineering and data preparation.\n",
      "It takes 39 sec.\n",
      "---=> final dataframe has 178 features <=---\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding                                                                                                                            \n",
    "df_dum = pd.get_dummies(df[['store', 'item', 'day_of_week', 'month', ]], columns=['store', 'item', 'day_of_week', 'month', ], dummy_na=True)  \n",
    "df = pd.concat([df, df_dum], axis=1)                                                                                                          \n",
    "\n",
    "# convert to logarithmic scale                                                                                                           \n",
    "df['sales'] = np.log1p(df[\"sales\"].values)\n",
    "\n",
    "print(f'End of feature engineering and data preparation.') \n",
    "print(f'It takes {int(time.time()-start_time)} sec.')\n",
    "print(f'---=> final dataframe has {df.shape[1]} features <=---') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-variable",
   "metadata": {
    "papermill": {
     "duration": 0.013552,
     "end_time": "2021-05-03T21:05:00.782876",
     "exception": false,
     "start_time": "2021-05-03T21:05:00.769324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LightGM Model with Final Dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dying-filename",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:05:00.822138Z",
     "iopub.status.busy": "2021-05-03T21:05:00.821407Z",
     "iopub.status.idle": "2021-05-03T21:25:48.114088Z",
     "shell.execute_reply": "2021-05-03T21:25:48.114826Z"
    },
    "papermill": {
     "duration": 1247.318074,
     "end_time": "2021-05-03T21:25:48.115078",
     "exception": false,
     "start_time": "2021-05-03T21:05:00.797004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model calculation starts..\n",
      "The model calculation is done in 1247 sec.\n"
     ]
    }
   ],
   "source": [
    "# MODEL VALIDATION\n",
    "start_time = time.time()\n",
    "print(\"Final model calculation starts..\")                                                                \n",
    "cols = [col for col in df.columns if col not in ['date', 'id', \"sales\", \"year\"]]                                                           \n",
    "\n",
    "train = df.loc[~df.sales.isna()]                                                                                                              \n",
    "X_train, Y_train = train[cols], train['sales']                                                                                                                         \n",
    "                                                                                                                                              \n",
    "test = df.loc[df.id.notnull()]                                                                                                                \n",
    "X_test = test[cols]                                                                                                                           \n",
    "                                                                                                                                              \n",
    "iteration = 15000\n",
    "                                                                                                       \n",
    "lgb_params = {                                                                                                                            \n",
    "        'nthread': -1,\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',    \n",
    "        'max_depth': 7,\n",
    "        'num_leaves': 28,   \n",
    "        'task': 'train',                                                                                                                      \n",
    "        'objective': 'regression_l1',                                                                                                         \n",
    "        'learning_rate': 0.05,                                                                                                                \n",
    "        'feature_fraction': 0.9,                                                                                                              \n",
    "        'bagging_fraction': 0.8,                                                                                                              \n",
    "        'bagging_freq': 5,                                                                                                                    \n",
    "        'lambda_l1': 0.06,                                                                                                                    \n",
    "        'lambda_l2': 0.05,                                                                                                                    \n",
    "        'verbose': -1,     }                                                                                                                           \n",
    "                                                                                                                                              \n",
    "# LightGBM dataset                                                                                                                        \n",
    "lgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)                                                                \n",
    "final_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=iteration)                                                              \n",
    "test_preds = final_model.predict(X_test, num_iteration=iteration)\n",
    "print(f'The model calculation is done in {int(time.time()-start_time)} sec.')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-vietnamese",
   "metadata": {
    "papermill": {
     "duration": 0.014247,
     "end_time": "2021-05-03T21:25:48.144753",
     "exception": false,
     "start_time": "2021-05-03T21:25:48.130506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating the Submission File :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "divine-retro",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T21:25:48.184869Z",
     "iopub.status.busy": "2021-05-03T21:25:48.183693Z",
     "iopub.status.idle": "2021-05-03T21:25:48.303666Z",
     "shell.execute_reply": "2021-05-03T21:25:48.304345Z"
    },
    "papermill": {
     "duration": 0.144533,
     "end_time": "2021-05-03T21:25:48.304549",
     "exception": false,
     "start_time": "2021-05-03T21:25:48.160016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Submission file is created!\n"
     ]
    }
   ],
   "source": [
    "# create submission file\n",
    "submission = pd.DataFrame({ 'id': [*range(45000)], 'sales': np.round(np.expm1(test_preds),0) }) # turn back to normal scale\n",
    "submission['sales'] = submission.sales.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f'OK, Submission file is created!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1302.639634,
   "end_time": "2021-05-03T21:25:50.222924",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-03T21:04:07.583290",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
